import cv2
import depthai as dai
import numpy as np
import time
import torch
import torchvision.transforms as transforms
from torchvision.models.video import r3d_18, R3D_18_Weights
from PIL import Image

# Constants
SEQUENCE_LENGTH = 16  # Number of frames in a sequence
CONFIDENCE_THRESHOLD = 0.5  # Violence detection threshold

# Load the R3D model with pre-trained weights
weights = R3D_18_Weights.KINETICS400_V1
model = r3d_18(weights=weights)
model.eval()

# Use GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Preprocessing pipeline for input frames
preprocess = transforms.Compose([
    transforms.Resize((128, 171)),
    transforms.CenterCrop((112, 112)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989])
])

# OAK-D Pipeline for RGB Camera
pipeline = dai.Pipeline()

# Setup RGB camera node
cam_rgb = pipeline.create(dai.node.ColorCamera)
cam_rgb.setBoardSocket(dai.CameraBoardSocket.CAM_A)  # Use CAM_A instead of deprecated RGB
cam_rgb.setPreviewSize(640, 480)
cam_rgb.setFps(30)
cam_rgb.setInterleaved(False)

# XLink output for video stream
xout_video = pipeline.create(dai.node.XLinkOut)
xout_video.setStreamName("video")
cam_rgb.video.link(xout_video.input)

# Function to preprocess and create a sequence of frames
def preprocess_frames(frame_list):
    processed_frames = []
    for frame in frame_list:
        # Convert NumPy array to PIL Image
        pil_image = Image.fromarray(frame)
        
        # Apply transformations
        processed_frame = preprocess(pil_image)
        processed_frames.append(processed_frame)
    
    # Stack frames along the depth dimension (2nd dimension for 3D convolution input)
    return torch.stack(processed_frames, dim=1).unsqueeze(0).to(device)

# Function to run violence detection using R3D model
def detect_violence(sequence):
    with torch.no_grad():
        outputs = model(sequence)
    probabilities = torch.nn.functional.softmax(outputs, dim=1)
    violence_class = 1  # Assuming violence is the class at index 1
    return probabilities[0, violence_class].item() > CONFIDENCE_THRESHOLD

def main():
    # Start the OAK-D device with the defined pipeline
    with dai.Device(pipeline) as device:
        # Output queues
        video_queue = device.getOutputQueue(name="video", maxSize=4, blocking=False)

        frame_buffer = []
        start_time = time.time()
        fps_counter = 0
        fps = 0

        while True:
            # Get frame from the OAK-D video stream
            video_packet = video_queue.get()
            frame = video_packet.getCvFrame()

            # Convert the frame to RGB (from BGR)
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # Add frame to the buffer
            frame_buffer.append(frame_rgb)

            # If we have enough frames for a sequence, run violence detection
            if len(frame_buffer) == SEQUENCE_LENGTH:
                # Preprocess the frames and run the detection
                frame_sequence = preprocess_frames(frame_buffer)
                is_violent = detect_violence(frame_sequence)

                # Display the result on the current frame
                label = "Violence Detected" if is_violent else "No Violence"
                color = (0, 0, 255) if is_violent else (0, 255, 0)
                cv2.putText(frame, label, (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)

                # Clear the buffer for the next sequence
                frame_buffer = []

            # Show FPS
            fps_counter += 1
            if (time.time() - start_time) > 1:
                fps = fps_counter / (time.time() - start_time)
                start_time = time.time()
                fps_counter = 0

            label_fps = "FPS: {:.2f}".format(fps)
            cv2.putText(frame, label_fps, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)

            # Display the frame
            cv2.imshow("Violence Detection", frame)

            # Exit if 'q' is pressed
            if cv2.waitKey(1) == ord('q'):
                break

        # Clean up
        cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
